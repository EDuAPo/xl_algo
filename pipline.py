import os
import sys
import subprocess
import yaml
import re
import shutil
import json
import tempfile
import time
from datetime import datetime
from typing import List, Tuple, Optional, Dict, Set

# ===================== é…ç½®åŒºåŸŸ =====================
# 1. æ ¸å¿ƒè„šæœ¬è·¯å¾„
FILTER_SCRIPT_PATH = "./move_file.py"  # ç­›é€‰è„šæœ¬
RUN_EXPORT_SCRIPT_PATH = "./run_export.py"  # é¢„å¤„ç†è„šæœ¬
CHECK_COMPRESS_SCRIPT_PATH = "./check_and_compress.py"  # æ£€æŸ¥å‹ç¼©è„šæœ¬

# 2. åŸºç¡€é…ç½®
DEFAULT_VEHICLE = "vehicle_000"
DEFAULT_MAIN_OUT = "/media/zgw/5211BF7864DFC4FA/1230out/"  # é¢„å¤„ç†ä¸»è¾“å‡ºç›®å½•
TIME_PERIODS_YAML = "./time_peridos.yaml"  # æ—¶é—´æ®µé…ç½®æ–‡ä»¶

# 3. æ–°å¢ï¼šç§»åŠ¨æ¨¡å¼é…ç½®
MOVE_MODE = True  # æ˜¯å¦ä½¿ç”¨ç§»åŠ¨æ¨¡å¼ï¼ˆé»˜è®¤Trueï¼Œæœ€èŠ‚çœç©ºé—´ï¼‰
MOVE_RECORD_DIR = "/media/zgw/5211BF7864DFC4FA/1230out/"  # ç§»åŠ¨è®°å½•ä¿å­˜ç›®å½•

# 4. æ–°å¢ï¼šæ£€æŸ¥å‹ç¼©åŠŸèƒ½é…ç½®
SKIP_CHECK_COMPRESS = False  # æ˜¯å¦è·³è¿‡å‹ç¼©æµç¨‹ï¼ˆé»˜è®¤ä¸è·³è¿‡ï¼‰
COMPRESS_FORMAT = "zip"  # å‹ç¼©æ ¼å¼
DELETE_RAW_UNDISTORTED = False  # å‹ç¼©åæ˜¯å¦åˆ é™¤åŸå§‹ undistorted ç›®å½•

# 5. æ–°å¢ï¼šsimple.json æ¸…ç†é…ç½®
CLEAN_BY_SIMPLE_JSON = True  # æ˜¯å¦æ ¹æ®simple.jsonæ¸…ç†æ–‡ä»¶
SIMPLE_JSON_NAME = "sample.json"  # simple.jsonæ–‡ä»¶å

# 6. å…¨å±€æ—¥å¿—æ•°æ®ç»“æ„
PIPELINE_LOG = {
    "session_info": {},
    "periods_processed": [],
    "summary": {}
}
SESSION_START_TIME = 0  # ä¼šè¯å¼€å§‹æ—¶é—´
# ===================================================

def save_pipeline_log(output_dir: str) -> None:
    """ä¿å­˜æµç¨‹æ—¥å¿—åˆ°JSONæ–‡ä»¶"""
    log_filename = "pipeline_log.json"
    log_path = os.path.join(output_dir, log_filename)
    try:
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(PIPELINE_LOG, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“ æµç¨‹æ—¥å¿—å·²ä¿å­˜: {log_filename}")
        return log_path
    except Exception as e:
        print(f"\nâš ï¸  ä¿å­˜æµç¨‹æ—¥å¿—å¤±è´¥: {e}")
        return None


def load_time_periods(yaml_path: str) -> List[Tuple[str, str]]:
    """ä»YAMLæ–‡ä»¶åŠ è½½æ—¶é—´æ®µåˆ—è¡¨"""
    if not os.path.exists(yaml_path):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°æ—¶é—´æ®µé…ç½®æ–‡ä»¶ï¼š{yaml_path}")
    
    with open(yaml_path, 'r', encoding='utf-8') as f:
        try:
            data = yaml.safe_load(f)
        except yaml.YAMLError as e:
            raise ValueError(f"YAMLæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼š{str(e)}")
    
    if not isinstance(data, list) or len(data) == 0:
        raise ValueError(f"YAMLæ–‡ä»¶å†…å®¹å¿…é¡»æ˜¯éç©ºåˆ—è¡¨")
    
    periods = []
    for idx, period in enumerate(data, 1):
        if not isinstance(period, list) or len(period) != 2:
            raise ValueError(f"YAMLç¬¬{idx}è¡Œæ ¼å¼é”™è¯¯ï¼šå¿…é¡»æ˜¯åŒ…å«2ä¸ªå…ƒç´ çš„åˆ—è¡¨")
        
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶è¡¥é›¶
        start = str(period[0]).zfill(6)
        end = str(period[1]).zfill(6)
        
        if not validate_time_format(start):
            raise ValueError(f"YAMLç¬¬{idx}è¡Œå¼€å§‹æ—¶é—´é”™è¯¯ï¼š{period[0]} ä¸æ˜¯æœ‰æ•ˆçš„HHMMSSæ ¼å¼")
        if not validate_time_format(end):
            raise ValueError(f"YAMLç¬¬{idx}è¡Œç»“æŸæ—¶é—´é”™è¯¯ï¼š{period[1]} ä¸æ˜¯æœ‰æ•ˆçš„HHMMSSæ ¼å¼")
        if start > end:
            raise ValueError(f"YAMLç¬¬{idx}è¡Œæ—¶é—´é”™è¯¯ï¼šç»“æŸæ—¶é—´ {end} æ—©äºå¼€å§‹æ—¶é—´ {start}")
        
        periods.append((start, end))
    
    return periods


def get_filter_script_config() -> tuple[str, str]:
    """ä» filter_by_time.py ä¸­è¯»å–çœŸå®çš„é»˜è®¤é…ç½®"""
    if not os.path.exists(FILTER_SCRIPT_PATH):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°ç­›é€‰è„šæœ¬ï¼š{FILTER_SCRIPT_PATH}")
    
    with open(FILTER_SCRIPT_PATH, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # åŒ¹é…é»˜è®¤é…ç½®ï¼ˆä¿®æ”¹åçš„æ¨¡å¼ï¼‰
    source_match = re.search(r'DEFAULT_SOURCE_DIRECTORY\s*=\s*"([^"]+)"', content)
    if not source_match:
        # å›é€€åˆ°æ—§æ¨¡å¼
        source_match = re.search(r'SOURCE_DIRECTORY\s*=\s*"([^"]+)"', content)
        if not source_match:
            raise ValueError(f"æœªåœ¨ {FILTER_SCRIPT_PATH} ä¸­æ‰¾åˆ°æºç›®å½•é…ç½®")
    
    output_match = re.search(r'DEFAULT_OUTPUT_ROOT_DIRECTORY\s*=\s*"([^"]+)"', content)
    if not output_match:
        # å›é€€åˆ°æ—§æ¨¡å¼
        output_match = re.search(r'OUTPUT_ROOT_DIRECTORY\s*=\s*"([^"]+)"', content)
        if not output_match:
            raise ValueError(f"æœªåœ¨ {FILTER_SCRIPT_PATH} ä¸­æ‰¾åˆ°è¾“å‡ºç›®å½•é…ç½®")
    
    return source_match.group(1).strip(), output_match.group(1).strip()


def run_shell_command(command: str, step_name: str) -> dict:
    """æ‰§è¡Œå‘½ä»¤ï¼Œå®æ—¶æ‰“å°æ—¥å¿—ï¼Œè¿”å›æ‰§è¡Œä¿¡æ¯"""
    start_time = time.time()
    print(f"\n{'='*60}")
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œï¼š{step_name}")
    print(f"å‘½ä»¤ï¼š{command}")
    print(f"{'='*60}")
    
    process = subprocess.Popen(
        command,
        shell=True,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        executable=os.environ.get('SHELL', '/bin/bash')
    )
    
    if process.stdout:
        for line in process.stdout:
            try:
                print(line.decode('utf-8', errors='ignore').strip())
            except Exception:
                print(line.decode(sys.getdefaultencoding(), errors='ignore').strip())
    
    process.wait()
    duration = time.time() - start_time
    
    result = {
        "step_name": step_name,
        "command": command,
        "return_code": process.returncode,
        "duration_seconds": round(duration, 2),
        "status": "success" if process.returncode == 0 else "failed"
    }
    
    if process.returncode != 0:
        print(f"\nâŒ æ­¥éª¤ [{step_name}] æ‰§è¡Œå¤±è´¥ï¼é”™è¯¯ç ï¼š{process.returncode}")
        raise RuntimeError(f"æ­¥éª¤ [{step_name}] æ‰§è¡Œå¤±è´¥ï¼é”™è¯¯ç ï¼š{process.returncode}")
    
    # ä¼˜åŒ–ï¼šå¼ºåˆ¶åŒæ­¥ç£ç›˜ï¼Œé˜²æ­¢IOç§¯å‹å¯¼è‡´åç»­æ­¥éª¤å˜æ…¢
    subprocess.run("sync", shell=True)
    
    print(f"\nâœ… æ­¥éª¤ [{step_name}] æ‰§è¡Œå®Œæˆï¼ï¼ˆè€—æ—¶: {duration:.2f}ç§’ï¼‰")
    return result


def get_filtered_folder_path(output_root: str, start_time: str, end_time: str) -> str:
    """æ ¹æ®ä½ çš„åŸæœ‰é€»è¾‘ï¼Œè®¡ç®—ç­›é€‰åçš„ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„"""
    return os.path.join(output_root, f"{start_time}_{end_time}")


def validate_time_format(time_str: str) -> bool:
    """éªŒè¯æ—¶é—´æ ¼å¼æ˜¯å¦ä¸º HHMMSSï¼ˆ6ä½æ•°å­—ï¼‰"""
    if len(time_str) != 6 or not time_str.isdigit():
        return False
    hh = int(time_str[:2])
    mm = int(time_str[2:4])
    ss = int(time_str[4:6])
    return 0 <= hh < 24 and 0 <= mm < 60 and 0 <= ss < 60


def modify_filter_script(start_time: str, end_time: str) -> None:
    """ä¿®æ”¹ filter_by_time.py çš„é»˜è®¤æ—¶é—´é…ç½®"""
    with open(FILTER_SCRIPT_PATH, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    updated_lines = []
    for line in lines:
        if line.strip().startswith("DEFAULT_TARGET_START_TIME"):
            updated_lines.append(f'DEFAULT_TARGET_START_TIME = "{start_time}"  # è‡ªåŠ¨æ›´æ–°äº {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n')
        elif line.strip().startswith("DEFAULT_TARGET_END_TIME"):
            updated_lines.append(f'DEFAULT_TARGET_END_TIME = "{end_time}"    # è‡ªåŠ¨æ›´æ–°äº {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n')
        # ä¹Ÿå¤„ç†æ—§çš„é…ç½®åç§°
        elif line.strip().startswith("TARGET_START_TIME") and not line.strip().startswith("DEFAULT_"):
            updated_lines.append(f'TARGET_START_TIME = "{start_time}"  # è‡ªåŠ¨æ›´æ–°äº {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n')
        elif line.strip().startswith("TARGET_END_TIME") and not line.strip().startswith("DEFAULT_"):
            updated_lines.append(f'TARGET_END_TIME = "{end_time}"    # è‡ªåŠ¨æ›´æ–°äº {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n')
        else:
            updated_lines.append(line)
    
    with open(FILTER_SCRIPT_PATH, 'w', encoding='utf-8') as f:
        f.writelines(updated_lines)
    
    print(f"âœ… å·²æ›´æ–°ç­›é€‰è„šæœ¬çš„æ—¶é—´æ®µé…ç½®ï¼š")
    print(f"   - å¼€å§‹æ—¶é—´ï¼š{start_time}ï¼ˆHHMMSSï¼‰")
    print(f"   - ç»“æŸæ—¶é—´ï¼š{end_time}ï¼ˆHHMMSSï¼‰")


def save_move_record(period_idx: int, start_time: str, end_time: str, moved_files: Dict[str, str]) -> str:
    """ä¿å­˜ç§»åŠ¨æ–‡ä»¶è®°å½•åˆ°JSONæ–‡ä»¶"""
    os.makedirs(MOVE_RECORD_DIR, exist_ok=True)
    
    record_filename = f"move_record_{period_idx:03d}_{start_time}_{end_time}.json"
    record_path = os.path.join(MOVE_RECORD_DIR, record_filename)
    
    with open(record_path, 'w', encoding='utf-8') as f:
        json.dump({
            'period_idx': period_idx,
            'start_time': start_time,
            'end_time': end_time,
            'moved_files': moved_files,
            'timestamp': datetime.now().isoformat()
        }, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“ å·²ä¿å­˜ç§»åŠ¨è®°å½•åˆ°ï¼š{record_path}")
    return record_path


def restore_moved_files(record_path: str) -> Tuple[int, int]:
    """æ ¹æ®è®°å½•æ–‡ä»¶æ¢å¤ç§»åŠ¨çš„æ–‡ä»¶ï¼Œè¿”å›ï¼ˆæˆåŠŸæ•°ï¼Œæ€»æ•°ï¼‰"""
    if not os.path.exists(record_path):
        print(f"âš ï¸  è®°å½•æ–‡ä»¶ä¸å­˜åœ¨ï¼š{record_path}")
        return 0, 0
    
    try:
        with open(record_path, 'r', encoding='utf-8') as f:
            record_data = json.load(f)
        
        moved_files = record_data['moved_files']
        total_files = len(moved_files)
        success_count = 0
        
        print(f"ğŸ”„ æ­£åœ¨æ¢å¤ {total_files} ä¸ªdb3æ–‡ä»¶...")
        
        for dest_path, src_path in moved_files.items():
            try:
                # æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆå³ç§»åŠ¨åçš„ä½ç½®ï¼‰
                if os.path.exists(dest_path):
                    # ç¡®ä¿æºç›®å½•å­˜åœ¨
                    src_dir = os.path.dirname(src_path)
                    os.makedirs(src_dir, exist_ok=True)
                    
                    # ç§»åŠ¨æ–‡ä»¶å›åŸå§‹ä½ç½®
                    shutil.move(dest_path, src_path)
                    # æ£€æŸ¥æ˜¯å¦æˆåŠŸç§»å›
                    if os.path.exists(src_path):
                        success_count += 1
                        print(f"   âœ… å·²æ¢å¤ï¼š{os.path.basename(dest_path)} -> {src_path}")
                    else:
                        print(f"   âŒ æ¢å¤å¤±è´¥ï¼šç§»åŠ¨æ“ä½œåæºæ–‡ä»¶ä¸å­˜åœ¨ {src_path}")
                else:
                    print(f"   âš ï¸  è·³è¿‡ï¼šæ–‡ä»¶ä¸å­˜åœ¨äºç›®æ ‡ä½ç½® {dest_path}")
            except Exception as e:
                print(f"   âŒ æ¢å¤å¤±è´¥ï¼š{os.path.basename(dest_path)} - {str(e)}")
        
        # åˆ é™¤è®°å½•æ–‡ä»¶
        os.remove(record_path)
        print(f"\nâœ… æ–‡ä»¶æ¢å¤å®Œæˆï¼šæˆåŠŸ {success_count}/{total_files}")
        
        return success_count, total_files
        
    except Exception as e:
        print(f"âŒ è¯»å–è®°å½•æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")
        return 0, 0


def cleanup_move_records():
    """æ¸…ç†æ‰€æœ‰ç§»åŠ¨è®°å½•æ–‡ä»¶"""
    if os.path.exists(MOVE_RECORD_DIR):
        try:
            shutil.rmtree(MOVE_RECORD_DIR)
            print(f"ğŸ—‘ï¸  å·²æ¸…ç†ç§»åŠ¨è®°å½•ç›®å½•ï¼š{MOVE_RECORD_DIR}")
        except Exception as e:
            print(f"âš ï¸  æ¸…ç†ç§»åŠ¨è®°å½•ç›®å½•å¤±è´¥ï¼š{str(e)}")


def find_undistorted_folder(preprocess_out_dir: str) -> Optional[str]:
    """åœ¨é¢„å¤„ç†è¾“å‡ºç›®å½•ä¸‹æŸ¥æ‰¾ undistorted æ–‡ä»¶å¤¹"""
    for root, dirs, files in os.walk(preprocess_out_dir):
        if "undistorted" in dirs:
            return os.path.join(root, "undistorted")
    return None


def run_check_and_compress(
    undistorted_path: str,
    compress_output_dir: str,
    period_idx: int,
    start_time: str,
    end_time: str
) -> str:
    """è°ƒç”¨å¤–éƒ¨æ£€æŸ¥å‹ç¼©è„šæœ¬ï¼Œæ‰§è¡Œå‹ç¼©æµç¨‹ï¼Œè¿”å›å‹ç¼©åŒ…è·¯å¾„"""
    # ä¿®æ”¹æ–‡ä»¶åæ ¼å¼ï¼šYYYYMMDD_HHMMSS-HHMMSS.zip
    current_date = datetime.now().strftime('%Y%m%d')
    compress_filename = f"{current_date}_{start_time}-{end_time}.{COMPRESS_FORMAT}"
    compress_path = os.path.join(compress_output_dir, compress_filename)
    
    check_compress_cmd = (
        f"{sys.executable} {CHECK_COMPRESS_SCRIPT_PATH} "
        f"--undistorted-path {undistorted_path} "
        f"--compress-path {compress_path} "
        f"--compress-format {COMPRESS_FORMAT} "
        f"--period {start_time}_{end_time}"
    )
    
    run_shell_command(
        check_compress_cmd,
        f"ç¬¬{period_idx}ä¸ªæ—¶é—´æ®µ - æ­¥éª¤4/4ï¼šæ£€æŸ¥+å‹ç¼©"
    )
    
    return compress_path


def delete_raw_undistorted(undistorted_path: str) -> None:
    """å‹ç¼©å®Œæˆåï¼Œåˆ é™¤åŸå§‹ undistorted ç›®å½•"""
    if DELETE_RAW_UNDISTORTED and os.path.exists(undistorted_path):
        try:
            shutil.rmtree(undistorted_path)
            print(f"âœ… å·²åˆ é™¤åŸå§‹ undistorted ç›®å½•ï¼š{undistorted_path}")
        except Exception as e:
            print(f"âš ï¸  åˆ é™¤åŸå§‹ undistorted ç›®å½•å¤±è´¥ï¼š{str(e)}")


def cleanup_by_simple_json(preprocess_out_dir: str, period_idx: int) -> dict:
    """æ ¹æ®simple.jsonæ¸…ç†æ–‡ä»¶ï¼Œè¿”å›æ¸…ç†ä¿¡æ¯"""
    start_time = time.time()
    print(f"\n{'='*60}")
    print(f"ğŸ§¹ å¼€å§‹æ ¹æ® simple.json æ¸…ç†æ–‡ä»¶ï¼ˆæ—¶é—´æ®µï¼š{period_idx}ï¼‰")
    print(f"{'='*60}")
    
    undistorted_path = find_undistorted_folder(preprocess_out_dir)
    if not undistorted_path:
        print(f"âš ï¸  æœªæ‰¾åˆ° undistorted æ–‡ä»¶å¤¹ï¼Œè·³è¿‡æ¸…ç†æ­¥éª¤")
        return {"status": "skipped", "reason": "undistorted folder not found", "duration_seconds": round(time.time() - start_time, 2)}
    
    json_path = os.path.join(undistorted_path, SIMPLE_JSON_NAME)
    if not os.path.exists(json_path):
        print(f"âš ï¸  æœªæ‰¾åˆ° {SIMPLE_JSON_NAME}ï¼Œè·³è¿‡æ¸…ç†æ­¥éª¤")
        return {"status": "skipped", "reason": "simple.json not found", "duration_seconds": round(time.time() - start_time, 2)}
    
    print(f"ğŸ“ undistorted ç›®å½•ï¼š{undistorted_path}")
    print(f"ğŸ“„ æ‰¾åˆ° simple.jsonï¼š{json_path}")
    
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        
        if not isinstance(json_data, list):
            print(f"âš ï¸  simple.json æ ¼å¼é”™è¯¯ï¼šæ ¹å…ƒç´ å¿…é¡»æ˜¯åˆ—è¡¨")
            return {"status": "failed", "reason": "invalid json format", "duration_seconds": round(time.time() - start_time, 2)}
        
        required_files = {}
        deleted_count = 0
        
        for item in json_data:
            for key, value in item.items():
                if (key.startswith("camera_") or key.startswith("iv_points_")) and value != "NOT_FOUND":
                    folder_name = key
                    if folder_name not in required_files:
                        required_files[folder_name] = set()
                    required_files[folder_name].add(value)
        
        if not required_files:
            print(f"âš ï¸  simple.json ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å­—æ®µï¼Œè·³è¿‡æ¸…ç†")
            return {"status": "skipped", "reason": "no valid fields in json", "duration_seconds": round(time.time() - start_time, 2)}
        
        print(f"ğŸ” è¯†åˆ«å‡º {len(required_files)} ä¸ªéœ€è¦æ¸…ç†çš„æ–‡ä»¶å¤¹")
        
        for folder_name, files_to_keep in required_files.items():
            folder_path = os.path.join(undistorted_path, folder_name)
            if not os.path.exists(folder_path):
                continue
            
            for root, dirs, filenames in os.walk(folder_path):
                for filename in filenames:
                    if filename.endswith('.npy'):
                        continue
                    
                    file_path = os.path.join(root, filename)
                    basename = os.path.basename(filename)
                    
                    should_delete = True
                    for required_file in files_to_keep:
                        if basename == required_file or basename in required_file or required_file in basename:
                            should_delete = False
                            break
                    
                    if should_delete:
                        try:
                            os.remove(file_path)
                            deleted_count += 1
                        except Exception:
                            pass
        
        print(f"\nâœ… æ¸…ç†å®Œæˆï¼æ€»å…±åˆ é™¤äº† {deleted_count} ä¸ªæ–‡ä»¶")
        return {
            "status": "success",
            "folders_cleaned": len(required_files),
            "files_deleted": deleted_count,
            "duration_seconds": round(time.time() - start_time, 2)
        }
        
    except Exception as e:
        print(f"âŒ æ¸…ç†è¿‡ç¨‹å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
        return {
            "status": "failed",
            "error": str(e),
            "duration_seconds": round(time.time() - start_time, 2)
        }


def process_single_period(
    period_idx: int,
    start_time: str,
    end_time: str,
    source_dir: str,
    output_root: str,
    logtime: str,
    vehicle: str,
    main_out: str
) -> dict:
    """å¤„ç†å•ä¸ªæ—¶é—´æ®µçš„å…¨æµç¨‹ï¼ˆç­›é€‰+é¢„å¤„ç†+æ¸…ç†+æ£€æŸ¥å‹ç¼©ï¼‰"""
    period_start_time = time.time()
    print(f"\n{'='*80}")
    print(f"ğŸ“Œ å¼€å§‹å¤„ç†ç¬¬ {period_idx}/{total_periods} ä¸ªæ—¶é—´æ®µï¼š{start_time} â†’ {end_time}")
    print(f"ğŸ“¦ æ–‡ä»¶æ¨¡å¼ï¼š{'ç§»åŠ¨' if MOVE_MODE else 'å¤åˆ¶'}")
    print(f"{'='*80}")
    
    # åˆå§‹åŒ–æ—¥å¿—è®°å½•
    period_log = {
        "period_index": f"{period_idx}/{total_periods}",
        "start_time": start_time,
        "end_time": end_time,
        "start_timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "status": "processing",
        "steps": []
    }
    
    # åˆå§‹åŒ–
    filtered_folder = get_filtered_folder_path(output_root, start_time, end_time)
    preprocess_out_dir = os.path.join(main_out, f"{start_time}_{end_time}")
    move_record_path = os.path.join("move_records", f"move_record_{start_time}_{end_time}.json")
    
    try:
        # 1. æ‰“å°é…ç½®ä¿¡æ¯
        print(f"\nğŸ“¥ æºdb3ç›®å½•ï¼š{source_dir}")
        print(f"ğŸ“¤ ç­›é€‰è¾“å‡ºç›®å½•ï¼š{filtered_folder}")
        print(f"âš™ï¸  é¢„å¤„ç†è¾“å‡ºç›®å½•ï¼š{preprocess_out_dir}")
        print(f"ğŸš— è½¦è¾†å‹å·ï¼š{vehicle}")
        print(f"â° æ—¥å¿—æ—¶é—´æˆ³ï¼š{logtime}")
        
        # 2. æ›´æ–°ç­›é€‰è„šæœ¬çš„æ—¶é—´æ®µ
        modify_filter_script(start_time, end_time)
        
        # 3. æ‰§è¡Œç­›é€‰db3æ–‡ä»¶ï¼ˆä½¿ç”¨ç§»åŠ¨æ¨¡å¼ï¼‰
        print(f"\nğŸ”§ å¼€å§‹ç­›é€‰æ­¥éª¤...")
        
        # åˆ›å»ºç§»åŠ¨è®°å½•æ–‡ä»¶è·¯å¾„
        if MOVE_MODE:
            move_record_path = os.path.join(tempfile.gettempdir(), f"move_{period_idx}_{start_time}_{end_time}.json")
        
        # æ„å»ºç­›é€‰å‘½ä»¤
        filter_cmd = (
            f"{sys.executable} {FILTER_SCRIPT_PATH} "
            f"--source {source_dir} "
            f"--output {output_root} "
            f"--start {start_time} "
            f"--end {end_time}"
        )
        
        if MOVE_MODE:
            filter_cmd += f" --move --save-record {move_record_path}"
        
        filter_result = run_shell_command(filter_cmd, f"ç¬¬{period_idx}ä¸ªæ—¶é—´æ®µ - æ­¥éª¤1/4ï¼šç­›é€‰db3æ–‡ä»¶")
        period_log["steps"].append(filter_result)
        
        # 4. æ£€æŸ¥ç­›é€‰ç»“æœ
        if not os.path.exists(filtered_folder):
            print(f"âŒ ç­›é€‰å¤±è´¥ï¼šæœªç”Ÿæˆç›®æ ‡æ–‡ä»¶å¤¹ {filtered_folder}")
            print(f"   è·³è¿‡å½“å‰æ—¶é—´æ®µï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª...")
            period_log["status"] = "failed"
            period_log["reason"] = "filter output folder not created"
            period_log["duration_seconds"] = round(time.time() - period_start_time, 2)
            period_log["end_timestamp"] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            return period_log
        
        # 5. æ‰§è¡Œé¢„å¤„ç†ï¼ˆè¿™ä¸€æ­¥éœ€è¦db3æ–‡ä»¶å­˜åœ¨ï¼‰
        print(f"\nâš™ï¸  å¼€å§‹é¢„å¤„ç†æ­¥éª¤...")
        run_export_cmd = (
            f"{sys.executable} {RUN_EXPORT_SCRIPT_PATH} "
            f"--bag {filtered_folder} "
            f"--out {preprocess_out_dir} "
            f"--vehicle {vehicle} "
            f"--logtime {logtime}"
        )
        export_result = run_shell_command(run_export_cmd, f"ç¬¬{period_idx}ä¸ªæ—¶é—´æ®µ - æ­¥éª¤2/4ï¼šé¢„å¤„ç†")
        period_log["steps"].append(export_result)
        
        print(f"âœ… é¢„å¤„ç†å®Œæˆï¼Œç°åœ¨å¯ä»¥å®‰å…¨æ¢å¤db3æ–‡ä»¶...")
        
        print(f"\nç¬¬{period_idx}ä¸ªæ—¶é—´æ®µ - æ­¥éª¤3/4ï¼šæ¢å¤ä¸æ¸…ç†")
        
        # 6. æ¢å¤db3æ–‡ä»¶ï¼ˆåœ¨é¢„å¤„ç†å®Œæˆåï¼‰
        restore_start = time.time()
        if MOVE_MODE and move_record_path and os.path.exists(move_record_path):
            print(f"\nğŸ”„ æ¢å¤db3æ–‡ä»¶åˆ°åŸå§‹ä½ç½®...")
            success_count, total_count = restore_moved_files(move_record_path)
            period_log["steps"].append({
                "step_name": "æ¢å¤db3æ–‡ä»¶",
                "status": "success" if success_count == total_count else "partial",
                "restored_files": success_count,
                "total_files": total_count,
                "duration_seconds": round(time.time() - restore_start, 2)
            })
            if success_count < total_count:
                print(f"âš ï¸  éƒ¨åˆ†æ–‡ä»¶æ¢å¤å¤±è´¥ï¼Œè¯·æ£€æŸ¥æºç›®å½•å’Œç›®æ ‡ç›®å½•")
        elif MOVE_MODE:
            print(f"âš ï¸  ç§»åŠ¨è®°å½•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•æ¢å¤db3æ–‡ä»¶")
            period_log["steps"].append({
                "step_name": "æ¢å¤db3æ–‡ä»¶",
                "status": "skipped",
                "reason": "no move record file",
                "duration_seconds": round(time.time() - restore_start, 2)
            })
        
        # 7. æ¸…ç†ä¸´æ—¶ç­›é€‰æ–‡ä»¶å¤¹ï¼ˆåªä¿ç•™metadata.yamlï¼‰
        if os.path.exists(filtered_folder):
            try:
                # åªåˆ é™¤db3æ–‡ä»¶ï¼Œä¿ç•™metadata.yaml
                for filename in os.listdir(filtered_folder):
                    if filename.endswith('.db3'):
                        os.remove(os.path.join(filtered_folder, filename))
                
                # å¦‚æœæ–‡ä»¶å¤¹ä¸ºç©ºï¼Œåˆ é™¤æ•´ä¸ªæ–‡ä»¶å¤¹
                if len(os.listdir(filtered_folder)) == 0:
                    os.rmdir(filtered_folder)
                    print(f"ğŸ—‘ï¸  å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤¹ï¼š{filtered_folder}")
            except Exception as e:
                print(f"âš ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤¹å¤±è´¥ï¼š{str(e)}")
        
        # 8. å…¶ä»–åç»­æ­¥éª¤
        if CLEAN_BY_SIMPLE_JSON:
            cleanup_result = cleanup_by_simple_json(preprocess_out_dir, period_idx)
            period_log["steps"].append({
                "step_name": "simple.jsonæ¸…ç†",
                **cleanup_result
            })
        
        # 9. æ£€æŸ¥å‹ç¼©æµç¨‹
        compress_path = None
        if not SKIP_CHECK_COMPRESS:
            undistorted_path = find_undistorted_folder(preprocess_out_dir)
            if undistorted_path:
                compress_start = time.time()
                compress_path = run_check_and_compress(
                    undistorted_path=undistorted_path,
                    compress_output_dir=preprocess_out_dir,
                    period_idx=period_idx,
                    start_time=start_time,
                    end_time=end_time
                )
                period_log["steps"].append({
                    "step_name": "æ£€æŸ¥+å‹ç¼©",
                    "status": "success" if compress_path and os.path.exists(compress_path) else "failed",
                    "compress_path": compress_path if compress_path else None,
                    "duration_seconds": round(time.time() - compress_start, 2)
                })
                delete_raw_undistorted(undistorted_path)
        
        # 10. æ‰“å°å®Œæˆä¿¡æ¯
        print(f"\nâœ… ç¬¬ {period_idx} ä¸ªæ—¶é—´æ®µå¤„ç†å®Œæˆï¼")
        print(f"   é¢„å¤„ç†ç»“æœï¼š{preprocess_out_dir}")
        if MOVE_MODE:
            print(f"   db3æ–‡ä»¶ï¼šå·²ç§»åŠ¨å¹¶æ¢å¤")
        if compress_path and os.path.exists(compress_path):
            print(f"   å‹ç¼©åŒ…ï¼š{compress_path}")
        
        # è®°å½•æˆåŠŸå®Œæˆ
        period_log["status"] = "success"
        period_log["output_dir"] = preprocess_out_dir
        if compress_path:
            period_log["compress_path"] = compress_path
        period_log["duration_seconds"] = round(time.time() - period_start_time, 2)
        period_log["end_timestamp"] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        return period_log
        
    except Exception as e:
        print(f"\nâŒ ç¬¬ {period_idx} ä¸ªæ—¶é—´æ®µå¤„ç†å¼‚å¸¸ï¼š{str(e)}")
        
        # å‘ç”Ÿå¼‚å¸¸æ—¶ä¹Ÿè¦å°è¯•æ¢å¤æ–‡ä»¶
        if MOVE_MODE and move_record_path and os.path.exists(move_record_path):
            print(f"ğŸ”„ å‘ç”Ÿå¼‚å¸¸ï¼Œå°è¯•æ¢å¤db3æ–‡ä»¶...")
            restore_moved_files(move_record_path)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(filtered_folder):
            try:
                shutil.rmtree(filtered_folder)
                print(f"ğŸ—‘ï¸  å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤¹ï¼š{filtered_folder}")
            except:
                pass
        
        # è®°å½•å¼‚å¸¸
        period_log["status"] = "failed"
        period_log["error"] = str(e)
        period_log["duration_seconds"] = round(time.time() - period_start_time, 2)
        period_log["end_timestamp"] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        return period_log


def main():
    global total_periods, SESSION_START_TIME
    SESSION_START_TIME = time.time()
    
    import argparse
    parser = argparse.ArgumentParser(description="ROS 2 Bag æ‰¹é‡æ—¶é—´ç­›é€‰ + é¢„å¤„ç† + æ£€æŸ¥å‹ç¼©å…¨æµç¨‹è„šæœ¬")
    parser.add_argument("--logtime", type=str, required=True, help="æ—¥å¿—æ—¶é—´æˆ³ï¼ˆå¦‚ï¼š20251124_111515ï¼Œç”¨äº run_export.pyï¼‰")
    parser.add_argument("--vehicle", type=str, default=DEFAULT_VEHICLE, help=f"è½¦è¾†å‹å·ï¼ˆé»˜è®¤ï¼š{DEFAULT_VEHICLE}ï¼‰")
    parser.add_argument("--main-out", type=str, default=DEFAULT_MAIN_OUT, help=f"é¢„å¤„ç†ä¸»è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼š{DEFAULT_MAIN_OUT}ï¼‰")
    parser.add_argument("--yaml-path", type=str, default=TIME_PERIODS_YAML, help=f"æ—¶é—´æ®µé…ç½®YAMLæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼š{TIME_PERIODS_YAML}ï¼‰")
    parser.add_argument("--skip-check-compress", action="store_true", help=f"è·³è¿‡æ£€æŸ¥å‹ç¼©æµç¨‹ï¼ˆé»˜è®¤ä¸è·³è¿‡ï¼Œä¼˜å…ˆçº§é«˜äºé…ç½®æ–‡ä»¶ï¼‰")
    parser.add_argument("--skip-clean-json", action="store_true", help=f"è·³è¿‡simple.jsonæ¸…ç†æµç¨‹ï¼ˆé»˜è®¤ä¸è·³è¿‡ï¼‰")
    parser.add_argument("--no-move", action="store_true", help=f"ç¦ç”¨ç§»åŠ¨æ¨¡å¼ï¼Œä½¿ç”¨å¤åˆ¶æ¨¡å¼ï¼ˆé»˜è®¤ä½¿ç”¨ç§»åŠ¨æ¨¡å¼ï¼‰")
    parser.add_argument("--clean-records", action="store_true", help=f"æ¸…ç†æ‰€æœ‰ç§»åŠ¨è®°å½•æ–‡ä»¶")
    args = parser.parse_args()
    
    # è¦†ç›–é…ç½®
    global MOVE_MODE, SKIP_CHECK_COMPRESS, CLEAN_BY_SIMPLE_JSON
    MOVE_MODE = not args.no_move
    if args.skip_check_compress:
        SKIP_CHECK_COMPRESS = True
    if args.skip_clean_json:
        CLEAN_BY_SIMPLE_JSON = False
    
    # æ¸…ç†ç§»åŠ¨è®°å½•ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.clean_records:
        cleanup_move_records()
        return
    
    # 1. åŠ è½½æ—¶é—´æ®µé…ç½®
    try:
        time_periods = load_time_periods(args.yaml_path)
        total_periods = len(time_periods)
        print(f"âœ… æˆåŠŸåŠ è½½ {total_periods} ä¸ªæ—¶é—´æ®µï¼š")
        for i, (start, end) in enumerate(time_periods, 1):
            print(f"   {i}. {start} â†’ {end}")
    except Exception as e:
        print(f"âŒ åŠ è½½æ—¶é—´æ®µé…ç½®å¤±è´¥ï¼š{str(e)}")
        sys.exit(1)
    
    # 2. è¯»å– filter_by_time.py çš„çœŸå®é…ç½®
    try:
        SOURCE_DIRECTORY, OUTPUT_ROOT_DIRECTORY = get_filter_script_config()
    except Exception as e:
        print(f"âŒ è¯»å–ç­›é€‰è„šæœ¬é…ç½®å¤±è´¥ï¼š{str(e)}")
        sys.exit(1)
    
    # 3. æ£€æŸ¥åŸºç¡€è·¯å¾„
    required_scripts = [
        (FILTER_SCRIPT_PATH, "ç­›é€‰è„šæœ¬"),
        (RUN_EXPORT_SCRIPT_PATH, "é¢„å¤„ç†è„šæœ¬"),
    ]
    if not SKIP_CHECK_COMPRESS:
        required_scripts.append((CHECK_COMPRESS_SCRIPT_PATH, "æ£€æŸ¥å‹ç¼©è„šæœ¬"))
    
    for script_path, script_name in required_scripts:
        if not os.path.exists(script_path):
            print(f"âŒ æœªæ‰¾åˆ°{script_name}ï¼š{script_path}")
            sys.exit(1)
    
    # 4. æ£€æŸ¥ç›®å½•
    if not os.path.exists(SOURCE_DIRECTORY):
        print(f"âŒ æºdb3ç›®å½•ä¸å­˜åœ¨ï¼š{SOURCE_DIRECTORY}")
        sys.exit(1)
    
    if not os.path.exists(OUTPUT_ROOT_DIRECTORY):
        print(f"âš ï¸  ç­›é€‰è¾“å‡ºæ ¹ç›®å½•ä¸å­˜åœ¨ï¼š{OUTPUT_ROOT_DIRECTORY}")
        print(f"   æ­£åœ¨è‡ªåŠ¨åˆ›å»ºè¯¥ç›®å½•...")
        try:
            os.makedirs(OUTPUT_ROOT_DIRECTORY, exist_ok=True)
            print(f"âœ… æˆåŠŸåˆ›å»ºç­›é€‰è¾“å‡ºæ ¹ç›®å½•ï¼š{OUTPUT_ROOT_DIRECTORY}")
        except Exception as e:
            print(f"âŒ åˆ›å»ºç­›é€‰è¾“å‡ºæ ¹ç›®å½•å¤±è´¥ï¼š{str(e)}")
            sys.exit(1)
    
    # 5. åˆ›å»ºä¸»è¾“å‡ºç›®å½•
    os.makedirs(args.main_out, exist_ok=True)
    
    # 6. è®°å½•ä¼šè¯ä¿¡æ¯åˆ°æ—¥å¿—
    PIPELINE_LOG["session_info"] = {
        "start_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "source_directory": SOURCE_DIRECTORY,
        "filter_output_root": OUTPUT_ROOT_DIRECTORY,
        "preprocess_output": args.main_out,
        "vehicle": args.vehicle,
        "logtime": args.logtime,
        "yaml_config": args.yaml_path,
        "total_periods": total_periods,
        "config": {
            "move_mode": MOVE_MODE,
            "move_record_dir": MOVE_RECORD_DIR if MOVE_MODE else None,
            "clean_by_simple_json": CLEAN_BY_SIMPLE_JSON,
            "skip_check_compress": SKIP_CHECK_COMPRESS,
            "compress_format": COMPRESS_FORMAT if not SKIP_CHECK_COMPRESS else None
        }
    }
    
    # æ‰“å°å…¨å±€é…ç½®ä¿¡æ¯
    print("\n========================================")
    print("ğŸ“‹ å…¨å±€é…ç½®ä¿¡æ¯")
    print("========================================")
    print(f"ğŸ“¥ æºdb3ç›®å½•ï¼š{SOURCE_DIRECTORY}")
    print(f"ğŸ“¤ ç­›é€‰è¾“å‡ºæ ¹ç›®å½•ï¼š{OUTPUT_ROOT_DIRECTORY}")
    print(f"âš™ï¸  é¢„å¤„ç†ä¸»è¾“å‡ºï¼š{args.main_out}")
    print(f"ğŸš— è½¦è¾†å‹å·ï¼š{args.vehicle}")
    print(f"â° æ—¥å¿—æ—¶é—´æˆ³ï¼š{args.logtime}")
    print(f"ğŸ“„ YAMLé…ç½®æ–‡ä»¶ï¼š{args.yaml_path}")
    print(f"ğŸ“¦ æ–‡ä»¶æ¨¡å¼ï¼š{'ç§»åŠ¨' if MOVE_MODE else 'å¤åˆ¶'}")
    if MOVE_MODE:
        print(f"ğŸ“ ç§»åŠ¨è®°å½•ç›®å½•ï¼š{MOVE_RECORD_DIR}")
    print(f"ğŸ§¹ simple.jsonæ¸…ç†ï¼š{'å¯ç”¨' if CLEAN_BY_SIMPLE_JSON else 'ç¦ç”¨'}")
    print(f"ğŸ—œï¸  æ£€æŸ¥å‹ç¼©æµç¨‹ï¼š{'å¯ç”¨' if not SKIP_CHECK_COMPRESS else 'ç¦ç”¨'}")
    print("========================================\n")
    
    # 7. æ‰¹é‡å¤„ç†æ¯ä¸ªæ—¶é—´æ®µ
    success_count = 0
    fail_count = 0
    
    for period_idx, (start_time, end_time) in enumerate(time_periods, 1):
        try:
            period_log = process_single_period(
                period_idx=period_idx,
                start_time=start_time,
                end_time=end_time,
                source_dir=SOURCE_DIRECTORY,
                output_root=OUTPUT_ROOT_DIRECTORY,
                logtime=args.logtime,
                vehicle=args.vehicle,
                main_out=args.main_out
            )
            PIPELINE_LOG["periods_processed"].append(period_log)
            if period_log["status"] == "success":
                success_count += 1
            else:
                fail_count += 1
        except Exception as e:
            print(f"\nâŒ ç¬¬ {period_idx} ä¸ªæ—¶é—´æ®µå¤„ç†å¼‚å¸¸ï¼š{str(e)}")
            print(f"   è·³è¿‡å½“å‰æ—¶é—´æ®µï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª...\n")
            fail_count += 1
            # è®°å½•å¼‚å¸¸çš„æ—¶é—´æ®µ
            PIPELINE_LOG["periods_processed"].append({
                "period_index": f"{period_idx}/{total_periods}",
                "start_time": start_time,
                "end_time": end_time,
                "status": "exception",
                "error": str(e)
            })
            continue
    
    # 8. è®°å½•æ±‡æ€»ä¿¡æ¯
    total_duration = time.time() - SESSION_START_TIME
    PIPELINE_LOG["summary"] = {
        "end_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "total_periods": total_periods,
        "successful_periods": success_count,
        "failed_periods": fail_count,
        "success_rate": f"{(success_count/total_periods*100):.2f}%" if total_periods > 0 else "0%",
        "total_duration_seconds": round(total_duration, 2),
        "total_duration_formatted": f"{int(total_duration//3600)}h {int((total_duration%3600)//60)}m {int(total_duration%60)}s",
        "average_time_per_period_seconds": round(total_duration / total_periods, 2) if total_periods > 0 else 0
    }
    
    # è¾“å‡ºæ€»ä½“ç»Ÿè®¡ç»“æœ
    print(f"\n{'='*80}")
    print("ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆï¼æ€»ä½“ç»Ÿè®¡ï¼š")
    print(f"   æ€»æ—¶é—´æ®µæ•°ï¼š{total_periods}")
    print(f"   æˆåŠŸå¤„ç†ï¼š{success_count} ä¸ª")
    print(f"   å¤±è´¥/è·³è¿‡ï¼š{fail_count} ä¸ª")
    print(f"   æˆåŠŸç‡ï¼š{PIPELINE_LOG['summary']['success_rate']}")
    print(f"   æ€»è€—æ—¶ï¼š{PIPELINE_LOG['summary']['total_duration_formatted']}")
    print(f"   å¹³å‡è€—æ—¶ï¼š{PIPELINE_LOG['summary']['average_time_per_period_seconds']:.2f}ç§’/æ—¶é—´æ®µ")
    print(f"ğŸ“ æ‰€æœ‰é¢„å¤„ç†ç»“æœå‡ä¿å­˜åœ¨ï¼š{args.main_out}")
    if MOVE_MODE:
        print(f"ğŸ“¦ ä½¿ç”¨ç§»åŠ¨æ¨¡å¼ï¼šdb3æ–‡ä»¶å·²å…¨éƒ¨æ¢å¤åŸå§‹ä½ç½®")
    print(f"{'='*80}")
    
    # ä¿å­˜æ—¥å¿—
    save_pipeline_log(args.main_out)


if __name__ == "__main__":
    main()
